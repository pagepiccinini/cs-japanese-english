---
title: 'Code-switching in Japanese-English bilinguals. Sub-project Discourse Markers:
  Pre-registration'
author: "Sho Tsuji & Page Piccinini"
date: "June 21, 2017"
header-includes:
  - \usepackage{tipa}
bibliography: cs_japanese_english_references.bib
cls: apa.csl
output:
  pdf_document: default
  html_document: default
---

# Introduction

Please refer to document "Code-switching in Japanese-English bilinguals: Preregistration document" for the general motivation and design of this project.
In this sub-project, we analyze discourse markers identified in the recordings based on the following first two prompts of the recording session:

1. Talk to each other about your experience living abroad and in Japan.
2. Talk to each other about your favorite US or English TV shows, for instance XXX.

The previous preregistration document included several target words for phonetic analysis, one of them, the discourse marker 'like', relevant to the first two prompts of the recording session. We had left open the possibility to identify and include other emergent frequently used discourse markers in the analysis. Having identified these markers, we will preregister this sub-project with a detailed analysis plan. 


# Research Questions

The specific goals of this subproject is to look at how the phonetics of discourse markers (originally English and Japanese) change according to the context in which they are produced. We want to see if a discourse marker, e.g. 'like', is: 1) produced differently in a monolingual-English versus monolingual-Japanese context; and if there is a difference between the two monolingual contexts, if it is 2) produced intermediate to the two languages in code-switching contexts.

# Data pre-processing

After data collection, we have conducted the following pre-processing steps in order to identify additional discourse markers.

**1. Selection of adequate conversations**

The conversation for each prompt was recorded in an individual soundfile, and speakers were separately recorded on the right and left channels. Thus, for each dyad, this subproject was based on 4 individual soundfiles (2 prompts x two speakers).

A total of nine dyads were recorded. Of these, we excluded three dyads, since they did not know each other before the session - one of the prerequisites we had set in order to ensure a natural conversation. In the included 6 dyads, one speaker of one dyad was not successfully recorded in one of the sequences. Thus, our analysis is based on 23 (6 dyads x 2 prompts x 2 speakers - 1) individual soundfiles.

**2. Annotation**

Conversations were transcribed in Praat in standard orthography appropriate for each language (using alphabet for English, katakana for Japanese). Separate segmentation tiers were used for each speaker for each language, resulting in four in total (Speaker #1 English, Speaker #1 Japanese, Speaker #2 English, Speaker #2 Japanese). Transcriptions were done by native speakers of each language. 

**3. Extraction of word and frequency count**

Based on orthographic transcriptions, we counted token frequency of individual lexical items. This frequency count showed that 'like' was the most frequently used lexical item in English, confirming the adequacy of including this discourse marker in the analysis. In addition, based on inspection of the most frequent words used, we decided to additionally include the words 'yeah', 'so'/'*so*'["yes" *particle*; "so/really" *adverb*] (which occurred both in English and Japanese), and '*nanka*' (["something/somehow"*adverb*]). These words were chosen based on their high frequency, their role as discourse markers, and their phonetic characteristics.


# Methods

Four words will be segmented for this analysis, 'like', 'yeah', 'so'/'*so*', and '*nanka*'. For each word the following phonemes will be segmented:

* 'like': [l], [a\textipa{I}], [k]-closure (if present), [k]-burst (if present)
* 'yeah': [j], [\textipa{\ae}]
* 'so'/'*so*: [s], [o\textipa{U}]
* '*nanka*': [na], [n], [ka]

Both duration and format values will be analyzed. For some sounds, formant values will be taken at several time points to see changes over time. For steady state vowels, formant values will be taken at the midpoint of the sound. See the "Analyses and predictions" section for more details.


# Analyses and predictions

In all analyses, we will compare the acoustic characteristics of the sounds as described below to determine whether they are:

1) produced differently in a monolingual-English versus monolingual-Japanese context, and
2) produced intermediate to the two languages in code-switching contexts.

For all analyses we will use linear mixed effects regression models (LMEMs). The summary of all models are provided below. A few general comments apply to the various analyses. One, we will only look at words directly at boundaries and thus are not predicted to be tagged to a specific language, so the fixed effects will be the language preceding and following the switch. Random effects structures below are maximal, but may need to change depending on what structure will result in a model that converges. Statistical testing will be done with model comparison and an alpha of 0.05.

## 'like'

* **[la\textipa{I}] duration**
    + \underline{Measurement:} Duration in milliseconds of [la\textipa{I}] portion of the token.
    + \underline{Prediction:} The duration is expected to be longer in Japanese contexts than English contexts, as English is expect to have a shorter diphthong, and in Japanese there may be two fully produced vowels, since Japanese only has monophthongs [@Okada1991].
    + \underline{Statistical analysis:}
```
lmer(lai_duration ~ language_proceeding * language_following +
(1 + language_proceeding * language_following | dyad/speaker))
```

* **[la\textipa{I}] formants**
    + \underline{Measurement:} F1, F2, and F3 measurements taken at 5% increments throughout the token.
    + \underline{Prediction:} We predict quality differences early on (during the [l]) as a postalveolar flap will be used in Japanese contexts, and an alveolar lateral approximant in English contexts, as Japanese lacks the lateral approximant [@Okada1991]. There will also be differences later on in the production, as English uses the lower and more central [\textipa{I}] vowel, while Japanese will use the higher and more fronted [i] vowel.
    + \underline{Statistical analyses (F3 used to normalize F1 and F2):}
```
lmer(lai_F1_normalized ~ language_proceeding * language_following * percentage +
(1 + langauge_proceeding * language_following_percentage | dyad/speaker))
```
```
lmer(lai_F2_normalized ~ language_proceeding * language_following * percentage +
(1 + langauge_proceeding * language_following_percentage | dyad/speaker))
```

* **[k]-closure presence **
    + \underline{Measurement:} Each token will be coded for whether the closure for the [k] is present or not.
    + \underline{Prediction:} The closure is predicted to be more likely in Japanese contexts than English contexts, as final stops are often unreleased in English [@Lisker1999].
    + \underline{Statistical analysis:}
```
glmer(k_closure_presence ~ language_proceeding * language_following +
(1 + langauge_proceeding * language_following | dyad/speaker), family = “binomial”)
```

* **[k]-burst presence **
    + \underline{Measurement:} Each token will be coded for whether the burst for the [k] is present or not.
    + \underline{Prediction:} The burst is predicted to be more likely in Japanese contexts than English contexts, as final stops are often unreleased in English [@Lisker1999].
    + \underline{Statistical analysis:}
```
glmer(k_burst_presence ~ language_proceeding * language_following +
(1 + langauge_proceeding * language_following | dyad/speaker), family = “binomial”)
```

* **[k]-burst duration **
    + \underline{Measurement:} If a burst is present, its position voice onset time (VOT) duration will be measured in milliseconds.
    + \underline{Prediction:} Japanese tokens are predicted to have a shorter duration, as the VOT of Japanese voiceless stops tend to be shorter than English voiceless stops [@Riney2007].
    + \underline{Statistical analysis:}
```
lmer(k_burst_duration ~ language_proceeding * language_following +
(1 + language_proceeding * language_following | dyad/speaker))
```

## 'yeah'

* **[j\textipa{\ae}] duration**
    + \underline{Measurement:} Duration in milliseconds of entire [j\textipa{\ae}] token.
    + \underline{Prediction:} There are no specific predictions for this analysis. The duration may be shorter in English if the final vowel takes on more of schwa quality.
    + \underline{Statistical analysis:}
```
lmer(jae_duration ~ language_proceeding * language_following +
(1 + language_proceeding * language_following | dyad/speaker))
```

* **[\textipa{\ae}] formants**
    + \underline{Measurement:} F1, F2, and F3 measurements at midpoint of [\textipa{\ae}].
    + \underline{Prediction:} We expect quality differences such that the vowel is an open central unrounded vowel in Japanese contexts [@Okada1991], and a near-open front unrounded vowel in English contexts [@Hillenbrand1995].
    + \underline{Statistical analyses (F3 used to normalize F1 and F2):}
```
lmer(ae_F1_midpoint_normalized ~ language_proceeding * language_following +
(1 + langauge_proceeding * language_following_percentage | dyad/speaker))
lmer(ae_F2_midpoint_normalized ~ language_proceeding * language_following +
(1 + langauge_proceeding * language_following_percentage | dyad/speaker))
```

## 'so'/'*so*'

* **[so\textipa{U}] duration**
    + \underline{Measurement:} Duration in milliseconds of entire [so\textipa{U}] token.
    + \underline{Prediction:} There are no specific predictions for this analysis. However, the duration may be longer in English since a diphthong will be produced, and a monophthong in Japanese
    + \underline{Statistical analysis:}
```
lmer(sou_duration ~ language_proceeding * language_following +
(1 + language_proceeding * language_following | dyad/speaker))
```

* **[o\textipa{U}] formants**
    + \underline{Measurement:} F1, F2, and F3 measurements taken at 5% increments throughout the token.
    + \underline{Prediction:}  We expect a flat vowel sound in Japanese context, and a diphthongized vowel in English contexts [@Hillenbrand1995].
    + \underline{Statistical analyses (F3 used to normalize F1 and F2):}
```
lmer(ow_F1_normalized ~ language_proceeding * language_following * percentage +
(1 + langauge_proceeding * language_following_percentage | dyad/speaker))
lmer(ow_F2_normalized ~ language_proceeding * language_following * percentage +
(1 + langauge_proceeding * language_following_percentage | dyad/speaker))
```

## '*nanka*'

* **[na] duration**
    + \underline{Measurement:} Duration in milliseconds of the initial [na] portion of the token.
    + \underline{Prediction:} Based on the languages' rhythmic structures, we predict the duration to be longer in Japanese contents, as the [na] will form a full unit, unlike in English where a coda is expected [@Kessler1997].
    + \underline{Statistical analysis:}
```
lmer(na_duration ~ language_proceeding * language_following +
(1 + language_proceeding * language_following | dyad/speaker))
```

* **[n] duration**
    + \underline{Measurement:} Duration in milliseconds of the second [n] portion of the token.
    + \underline{Prediction:} Based on the languages' rhythmic structures, we predict that the moraic nasal [n] will be longer in Japanese [@Otake1996] than in English contexts.
    + \underline{Statistical analysis:}
```
lmer(n_duration ~ language_proceeding * language_following +
(1 + language_proceeding * language_following | dyad/speaker))
```

* **[ka] duration**
    + \underline{Measurement:} Duration in milliseconds of final [ka] portion of the token.
    + \underline{Prediction:} There are no specific predictions for this analysis, as [ka] could function as a unit in both a Japanese or an English context.
    + \underline{Statistical analysis:}
```
lmer(ka_duration ~ language_proceeding * language_following +
(1 + language_proceeding * language_following | dyad/speaker))
```

*PAGE NOTE: WE COULD DO THE FOLLOWING TWO ANALYSES INSTEAD OF THE FIRST THREE.*

* **Moraic partition standard deviation**
    + \underline{Measurement:} For a given token, we will compute the standard deviation in milliseconds of the three units of the token given a moraic partition, [na] [n] [ka].
    + \underline{Prediction:} The standard deviation should be smaller in Japanese contexts compared to English contexts, since Japanese follows a moraic structure [@Otake1996].
    + \underline{Statistical analysis:}
```
lmer(moraic_sd ~ language_proceeding * language_following +
(1 + language_proceeding * language_following | dyad/speaker))
```
    
* **Syllabic partition standard deviation**
    + \underline{Measurement:} For a given token, we will compute the standard deviation in milliseconds of the two units of the token given a syllabic partition, [nan] [ka].
    + \underline{Prediction:} The standard deviation should be smaller in English contexts compared to Japanese contexts, since English follows syllabic structure [@Kessler1997].
    + \underline{Statistical analysis:}
```
lmer(syllabic_sd ~ language_proceeding * language_following +
(1 + language_proceeding * language_following | dyad/speaker))
```

# References